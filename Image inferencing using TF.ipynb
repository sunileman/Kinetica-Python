{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This will demenstate how to pull images stored in Kinetica, run Tensorflow inferencing against the images, and store results back into Kinetica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import os.path\n",
    "import re\n",
    "import sys\n",
    "import tarfile\n",
    "\n",
    "import numpy as np\n",
    "from six.moves import urllib\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLAGS = None\n",
    "\n",
    "# pylint: disable=line-too-long\n",
    "DATA_URL = 'http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz'\n",
    "# pylint: enable=line-too-long\n",
    "\n",
    "\n",
    "my_collection = \"MASTER\"\n",
    "inferenced_table = \"caltech_inferenced\"\n",
    "image_count=0\n",
    "images_table=\"caltech256\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NodeLookup(object):\n",
    "  \"\"\"Converts integer node ID's to human readable labels.\"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               label_lookup_path=None,\n",
    "               uid_lookup_path=None):\n",
    "    if not label_lookup_path:\n",
    "      label_lookup_path = os.path.join(\n",
    "          '/tmp/imagenet', 'imagenet_2012_challenge_label_map_proto.pbtxt')\n",
    "    if not uid_lookup_path:\n",
    "      uid_lookup_path = os.path.join(\n",
    "          '/tmp/imagenet', 'imagenet_synset_to_human_label_map.txt')\n",
    "    self.node_lookup = self.load(label_lookup_path, uid_lookup_path)\n",
    "\n",
    "  def load(self, label_lookup_path, uid_lookup_path):\n",
    "    \"\"\"Loads a human readable English name for each softmax node.\n",
    "    Args:\n",
    "      label_lookup_path: string UID to integer node ID.\n",
    "      uid_lookup_path: string UID to human-readable string.\n",
    "    Returns:\n",
    "      dict from integer node ID to human-readable string.\n",
    "    \"\"\"\n",
    "    if not tf.gfile.Exists(uid_lookup_path):\n",
    "      tf.logging.fatal('File does not exist %s', uid_lookup_path)\n",
    "    if not tf.gfile.Exists(label_lookup_path):\n",
    "      tf.logging.fatal('File does not exist %s', label_lookup_path)\n",
    "\n",
    "    # Loads mapping from string UID to human-readable string\n",
    "    proto_as_ascii_lines = tf.gfile.GFile(uid_lookup_path).readlines()\n",
    "    uid_to_human = {}\n",
    "    p = re.compile(r'[n\\d]*[ \\S,]*')\n",
    "    for line in proto_as_ascii_lines:\n",
    "      parsed_items = p.findall(line)\n",
    "      uid = parsed_items[0]\n",
    "      human_string = parsed_items[2]\n",
    "      uid_to_human[uid] = human_string\n",
    "\n",
    "    # Loads mapping from string UID to integer node ID.\n",
    "    node_id_to_uid = {}\n",
    "    proto_as_ascii = tf.gfile.GFile(label_lookup_path).readlines()\n",
    "    for line in proto_as_ascii:\n",
    "      if line.startswith('  target_class:'):\n",
    "        target_class = int(line.split(': ')[1])\n",
    "      if line.startswith('  target_class_string:'):\n",
    "        target_class_string = line.split(': ')[1]\n",
    "        node_id_to_uid[target_class] = target_class_string[1:-2]\n",
    "\n",
    "    # Loads the final mapping of integer node ID to human-readable string\n",
    "    node_id_to_name = {}\n",
    "    for key, val in node_id_to_uid.items():\n",
    "      if val not in uid_to_human:\n",
    "        tf.logging.fatal('Failed to locate: %s', val)\n",
    "      name = uid_to_human[val]\n",
    "      node_id_to_name[key] = name\n",
    "\n",
    "    return node_id_to_name\n",
    "\n",
    "  def id_to_string(self, node_id):\n",
    "    if node_id not in self.node_lookup:\n",
    "      return ''\n",
    "    return self.node_lookup[node_id]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph():\n",
    "  \"\"\"Creates a graph from saved GraphDef file and returns a saver.\"\"\"\n",
    "  # Creates graph from saved graph_def.pb.\n",
    "  with tf.gfile.FastGFile(os.path.join(\n",
    "      \"/tmp/imagenet\", 'classify_image_graph_def.pb'), 'rb') as f:\n",
    "    graph_def = tf.GraphDef()\n",
    "    graph_def.ParseFromString(f.read())\n",
    "    _ = tf.import_graph_def(graph_def, name='')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference_on_image(images,primaryKeys):\n",
    "  \"\"\"Runs inference on an image.\n",
    "  Args:\n",
    "    image: Image file name.\n",
    "  Returns:\n",
    "    Nothing\n",
    "  \"\"\"\n",
    "  import time\n",
    "  images_data = images\n",
    "\n",
    "\n",
    "\n",
    "  with tf.Session() as sess:\n",
    "    encoded_obj_list = []\n",
    "    # Some useful tensors:\n",
    "    # 'softmax:0': A tensor containing the normalized prediction across\n",
    "    #   1000 labels.\n",
    "    # 'pool_3:0': A tensor containing the next-to-last layer containing 2048\n",
    "    #   float description of the image.\n",
    "    # 'DecodeJpeg/contents:0': A tensor containing a string providing JPEG\n",
    "    #   encoding of the image.\n",
    "    # Runs the softmax tensor by feeding the image_data as input to the graph.\n",
    "    softmax_tensor = sess.graph.get_tensor_by_name('softmax:0')\n",
    "    \n",
    "\n",
    "    start = time.time()\n",
    "    \n",
    "    for ix in range(len(images_data)):\n",
    "        predictions = sess.run(softmax_tensor,\n",
    "                               {'DecodeJpeg/contents:0': images_data[ix]})\n",
    "        predictions = np.squeeze(predictions)\n",
    "\n",
    "        # Creates node ID --> English string lookup.\n",
    "        node_lookup = NodeLookup()\n",
    "\n",
    "       # top_k = predictions.argsort()[-FLAGS.num_top_predictions:][::-1]\n",
    "        top_k = predictions.argsort()[-5:][::-1]\n",
    "\n",
    "\n",
    "\n",
    "        for node_id in top_k:\n",
    "          human_string = node_lookup.id_to_string(node_id)\n",
    "          score = predictions[node_id]\n",
    "          splits = human_string.split(',')\n",
    "          for inference in splits:\n",
    "            encoded_obj_list.append(gpudb.GPUdbRecord(image_inference_record_type, [primaryKeys[ix], inference, score.item()]).binary_data)\n",
    "          #print('%s (score = %.5f)' % (human_string, score))   \n",
    "    \n",
    "    end = time.time()\n",
    "    elapsed = end - start\n",
    "    \n",
    "    print(\"Total model run time: \"+str(elapsed))\n",
    "    \n",
    "    writeToKinetica(encoded_obj_list)\n",
    "            \n",
    "    \n",
    "    #return inference, scores\n",
    "      #print(score)\n",
    "      #print('%s (score = %.5f)' % (human_string, score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maybe_download_and_extract():\n",
    "  \"\"\"Download and extract model tar file.\"\"\"\n",
    "  dest_directory = \"/tmp/imagenet\"\n",
    "  if not os.path.exists(dest_directory):\n",
    "    os.makedirs(dest_directory)\n",
    "  filename = DATA_URL.split('/')[-1]\n",
    "  filepath = os.path.join(dest_directory, filename)\n",
    "  if not os.path.exists(filepath):\n",
    "    def _progress(count, block_size, total_size):\n",
    "      sys.stdout.write('\\r>> Downloading %s %.1f%%' % (\n",
    "          filename, float(count * block_size) / float(total_size) * 100.0))\n",
    "      sys.stdout.flush()\n",
    "    filepath, _ = urllib.request.urlretrieve(DATA_URL, filepath, _progress)\n",
    "    print()\n",
    "    statinfo = os.stat(filepath)\n",
    "    print('Successfully downloaded', filename, statinfo.st_size, 'bytes.')\n",
    "  tarfile.open(filepath, 'r:gz').extractall(dest_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    " maybe_download_and_extract()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create table to store inference output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<gpudb.GPUdbRecordType object at 0x7fe3270ca3d0>\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/opt/gpudb/api/python/gpudb/')\n",
    "sys.path.insert(0, '/opt/gpudb/api/python/gpudb/packages')\n",
    "import base64\n",
    "import gpudb\n",
    "import numpy\n",
    "\n",
    "gpudb_host = \"127.0.0.1\"\n",
    "my_collection = \"MASTER\"\n",
    "my_table = \"caltech256\"\n",
    "h_db = gpudb.GPUdb(encoding = 'BINARY', host = gpudb_host, port = '9191')\n",
    "\n",
    "columns = []\n",
    "columns.append(gpudb.GPUdbRecordColumn(\"id\", gpudb.GPUdbRecordColumn._ColumnType.STRING, [gpudb.GPUdbColumnProperty.CHAR16, gpudb.GPUdbColumnProperty.SHARD_KEY]))\n",
    "columns.append(gpudb.GPUdbRecordColumn(\"inference\", gpudb.GPUdbRecordColumn._ColumnType.STRING, [gpudb.GPUdbColumnProperty.CHAR64]))\n",
    "columns.append(gpudb.GPUdbRecordColumn(\"score\", gpudb.GPUdbRecordColumn._ColumnType.FLOAT))\n",
    "# Create the type object\n",
    "image_inference_record_type = gpudb.GPUdbRecordType(columns, label=\"image_inference\")\n",
    "print(image_inference_record_type)\n",
    "\n",
    "\"\"\" Create the type in the database and save the type ID, needed to create\n",
    "    a table in the next step \"\"\"\n",
    "image_inference_record_type.create_type(h_db)\n",
    "image_type_id = image_inference_record_type.type_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status_info': {u'data_type': u'clear_table_response_avro',\n",
       "  u'message': u'',\n",
       "  'response_time': 0.00688,\n",
       "  u'status': u'OK'},\n",
       " u'table_name': u'caltech_inferenced'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_db.clear_table( table_name = inferenced_table, authorization = '', options = {} )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "response = h_db.create_table(table_name=inferenced_table, type_id=image_type_id,options = {\"collection_name\":my_collection})\n",
    "print(response['status_info']['status'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeToKinetica(encoded_obj_list):\n",
    "    import time\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    print(\"writing\")\n",
    "    response = h_db.insert_records(table_name=inferenced_table, data=encoded_obj_list, list_encoding=\"binary\", options={})\n",
    "    \n",
    "    print(response)\n",
    "    \n",
    "    end = time.time()\n",
    "    elapsed = end - start\n",
    "    \n",
    "    print(\"Total write time: \"+str(elapsed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find total count for image table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images in caltech256: 30607\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/opt/gpudb/api/python/gpudb/')\n",
    "sys.path.insert(0, '/opt/gpudb/api/python/gpudb/packages')\n",
    "import base64\n",
    "import gpudb\n",
    "import numpy\n",
    "import datetime\n",
    "\n",
    "gpudb_host = \"127.0.0.1\"\n",
    "my_collection = \"MASTER\"\n",
    "my_table = \"caltech256\"\n",
    "h_db = gpudb.GPUdb(encoding = 'BINARY', host = gpudb_host, port = '9191')\n",
    "\n",
    "response=h_db.aggregate_group_by(table_name=images_table, column_names=['count(*) AS c'], offset=0, limit=10, encoding=\"binary\", options={})\n",
    "tcount = gpudb.GPUdbRecord.decode_binary_data(response[\"response_schema_str\"], response[\"binary_encoded_response\"])\n",
    "\n",
    "for k, v in tcount.items():\n",
    "    if str(k) == 'column_1':\n",
    "        image_count=v[0]\n",
    "        print(\"Total images in \"+images_table+\": \"+str(image_count))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop over images and run inference and output in db/engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of images to process: 30607\n",
      "Number of images fetched: 10000\n",
      "Total model run time: 1844.32697392\n",
      "writing\n",
      "{u'count_updated': 0, u'record_ids': [], 'status_info': {u'status': u'OK', u'data_type': u'insert_records_response_avro', u'message': u'', 'response_time': 0.12553}, u'count_inserted': 93925}\n",
      "Total write time: 0.73143696785\n",
      "Number of images processed: 10000\n",
      "Total remaining images to process: 20607\n",
      "Number of images fetched: 10000\n",
      "Total model run time: 1869.92629504\n",
      "writing\n",
      "{u'count_updated': 0, u'record_ids': [], 'status_info': {u'status': u'OK', u'data_type': u'insert_records_response_avro', u'message': u'', 'response_time': 0.11456}, u'count_inserted': 93906}\n",
      "Total write time: 0.708550930023\n",
      "Number of images processed: 20000\n",
      "Total remaining images to process: 10607\n",
      "Number of images fetched: 10000\n",
      "Total model run time: 1885.29081082\n",
      "writing\n",
      "{u'count_updated': 0, u'record_ids': [], 'status_info': {u'status': u'OK', u'data_type': u'insert_records_response_avro', u'message': u'', 'response_time': 0.11307}, u'count_inserted': 94079}\n",
      "Total write time: 0.721863031387\n",
      "Number of images processed: 30000\n",
      "Total remaining images to process: 607\n",
      "Number of images fetched: 606\n",
      "Total model run time: 112.476008892\n",
      "writing\n",
      "{u'count_updated': 0, u'record_ids': [], 'status_info': {u'status': u'OK', u'data_type': u'insert_records_response_avro', u'message': u'', 'response_time': 0.01037}, u'count_inserted': 5630}\n",
      "Total write time: 0.0469892024994\n",
      "Number of images processed: 30606\n",
      "Total remaining images to process: 1\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'type_schema'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-6834f120e0ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh_db\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtable_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'caltech256'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimages_processed\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0mres_decoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgpudb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGPUdbRecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode_binary_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"type_schema\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"records_binary\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Number of images fetched: \"\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres_decoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'type_schema'"
     ]
    }
   ],
   "source": [
    "from multiprocessing.dummy import Pool as ThreadPool \n",
    "import sys\n",
    "sys.path.insert(0, '/opt/gpudb/api/python/gpudb/')\n",
    "sys.path.insert(0, '/opt/gpudb/api/python/gpudb/packages')\n",
    "import base64\n",
    "import gpudb\n",
    "import numpy\n",
    "import datetime\n",
    "\n",
    "\n",
    "#pool = ThreadPool(16) \n",
    "\n",
    "\n",
    "gpudb_host = \"127.0.0.1\"\n",
    "my_collection = \"MASTER\"\n",
    "my_table = \"caltech256\"\n",
    "h_db = gpudb.GPUdb(encoding = 'BINARY', host = gpudb_host, port = '9191')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# define worker function before a Pool is instantiated\n",
    "'''def worker(record):\n",
    "    primary_k=None\n",
    "    gimage=None\n",
    "    try:\n",
    "        encoded_obj_list = []\n",
    "        \n",
    "        for k, v in record.items():\n",
    "            if str(k) == 'id':\n",
    "                primary_k=v\n",
    "            if str(k) == 'image':\n",
    "                gimage=v\n",
    "        a,b = run_inference_on_image(gimage)\n",
    "\n",
    "        for ix in range(len(a)):\n",
    "            encoded_obj_list.append(gpudb.GPUdbRecord(image_inference_record_type, [primary_k, a[ix], b[ix].item()]).binary_data)\n",
    "        response = h_db.insert_records(table_name=inferenced_table, data=encoded_obj_list, list_encoding=\"binary\", options={})\n",
    "        #print(response)\n",
    "    except Exception as e: print(e)  \n",
    "'''\n",
    "        \n",
    "#keep count of number of images processed for while loop\n",
    "images_processed=0\n",
    "\n",
    "print(\"Total number of images to process: \"+str(image_count))\n",
    "\n",
    "\n",
    "\n",
    "while (image_count > images_processed ):\n",
    "\n",
    "    response = h_db.get_records(table_name='caltech256',offset=images_processed,limit=10000)\n",
    "    res_decoded = gpudb.GPUdbRecord.decode_binary_data(response[\"type_schema\"], response[\"records_binary\"])\n",
    "    \n",
    "    print(\"Number of images fetched: \"+ str(len(res_decoded)))\n",
    "    \n",
    "    glist=[]\n",
    "    pklist=[]\n",
    "    \n",
    "    for index in range(len(res_decoded)):\n",
    "        for k, v in res_decoded[index].items():\n",
    "            if str(k) == 'id':\n",
    "                pklist.append(v)\n",
    "            if str(k) == 'image':\n",
    "                glist.append(v)\n",
    "\n",
    "    create_graph()\n",
    "    run_inference_on_image(glist,pklist)\n",
    "    \n",
    "    images_processed+=len(res_decoded)\n",
    "    print(\"Number of images processed: \"+str(images_processed))\n",
    "    print(\"Total remaining images to process: \" + str(image_count-images_processed))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing..ignore below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''from multiprocessing.dummy import Pool as ThreadPool \n",
    "import sys\n",
    "sys.path.insert(0, '/opt/gpudb/api/python/gpudb/')\n",
    "sys.path.insert(0, '/opt/gpudb/api/python/gpudb/packages')\n",
    "import base64\n",
    "import gpudb\n",
    "import numpy\n",
    "import datetime\n",
    "\n",
    "\n",
    "pool = ThreadPool(16) \n",
    "\n",
    "\n",
    "gpudb_host = \"127.0.0.1\"\n",
    "my_collection = \"MASTER\"\n",
    "my_table = \"caltech256\"\n",
    "h_db = gpudb.GPUdb(encoding = 'BINARY', host = gpudb_host, port = '9191')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# define worker function before a Pool is instantiated\n",
    "def worker(record):\n",
    "    primary_k=None\n",
    "    gimage=None\n",
    "    try:\n",
    "        encoded_obj_list = []\n",
    "        \n",
    "        for k, v in record.items():\n",
    "            if str(k) == 'id':\n",
    "                primary_k=v\n",
    "            if str(k) == 'image':\n",
    "                gimage=v\n",
    "        a,b = run_inference_on_image(gimage)\n",
    "\n",
    "        for ix in range(len(a)):\n",
    "            encoded_obj_list.append(gpudb.GPUdbRecord(image_inference_record_type, [primary_k, a[ix], b[ix].item()]).binary_data)\n",
    "        response = h_db.insert_records(table_name=inferenced_table, data=encoded_obj_list, list_encoding=\"binary\", options={})\n",
    "        #print(response)\n",
    "    except Exception as e: print(e)  \n",
    "        \n",
    "        \n",
    "\n",
    "response = h_db.get_records(table_name='caltech256',offset=0,limit=image_count)\n",
    "res_decoded = gpudb.GPUdbRecord.decode_binary_data(response[\"type_schema\"], response[\"records_binary\"])\n",
    "\n",
    "print(len(res_decoded))\n",
    "\n",
    "encoded_obj_list = []\n",
    "\n",
    "create_graph()\n",
    "\n",
    "results = pool.map(worker, res_decoded)\n",
    "\n",
    "\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "    #for ix in range(len(inference)):\n",
    "    #    val1 = numpy.asscalar(scores[ix])\n",
    "    #    encoded_obj_list.append(gpudb.GPUdbRecord(image_inference_record_type, [primary_k, inference[ix], val1]).binary_data)           \n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''from multiprocessing.pool import ThreadPool as Pool\n",
    "import sys\n",
    "sys.path.insert(0, '/opt/gpudb/api/python/gpudb/')\n",
    "sys.path.insert(0, '/opt/gpudb/api/python/gpudb/packages')\n",
    "import base64\n",
    "import gpudb\n",
    "import numpy\n",
    "import datetime\n",
    "\n",
    "\n",
    "pool_size = 20  # your \"parallelness\"\n",
    "\n",
    "\n",
    "gpudb_host = \"127.0.0.1\"\n",
    "my_collection = \"MASTER\"\n",
    "my_table = \"caltech256\"\n",
    "h_db = gpudb.GPUdb(encoding = 'BINARY', host = gpudb_host, port = '9191')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# define worker function before a Pool is instantiated\n",
    "def worker(record):\n",
    "    primary_k=None\n",
    "    gimage=None\n",
    "    try:\n",
    "        encoded_obj_list = []\n",
    "        create_graph()\n",
    "        \n",
    "        for k, v in record.items():\n",
    "            if str(k) == 'id':\n",
    "                primary_k=v\n",
    "            if str(k) == 'image':\n",
    "                gimage=v\n",
    "        a,b = run_inference_on_image(gimage)\n",
    "\n",
    "        for ix in range(len(a)):\n",
    "            encoded_obj_list.append(gpudb.GPUdbRecord(image_inference_record_type, [primary_k, a[ix], b[ix].item()]).binary_data)\n",
    "        response = h_db.insert_records(table_name=inferenced_table, data=encoded_obj_list, list_encoding=\"binary\", options={})\n",
    "        #print(response)\n",
    "    except Exception as e: print(e)  \n",
    "        \n",
    "        \n",
    "        \n",
    "pool = Pool(pool_size)\n",
    "\n",
    "print(pool.is_alive())\n",
    "\n",
    "\n",
    "response = h_db.get_records(table_name='caltech256',offset=0,limit=image_count)\n",
    "res_decoded = gpudb.GPUdbRecord.decode_binary_data(response[\"type_schema\"], response[\"records_binary\"])\n",
    "\n",
    "\n",
    "encoded_obj_list = []\n",
    "\n",
    "\n",
    "for index in range(image_count):\n",
    "    pool.apply_async(worker, (res_decoded[index],))\n",
    "           #run_inference_on_image(gimage)\n",
    "        \n",
    "\n",
    "\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "    #for ix in range(len(inference)):\n",
    "    #    val1 = numpy.asscalar(scores[ix])\n",
    "    #    encoded_obj_list.append(gpudb.GPUdbRecord(image_inference_record_type, [primary_k, inference[ix], val1]).binary_data)          \n",
    "    \n",
    "    '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
